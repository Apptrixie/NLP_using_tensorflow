Results are discussed in this file.

We started the exercise by learning how tokenization of text is done. We did this by converting words of sentence sequence to tokens by using the Tokenizer object.
We also learned how to convert sentences into data, by forming padded arrays for all sentence sequence. 

After learning, basic pre-processing of text, we applied the same techniques on a dataset of news headlines and try to classify them as sarcastic or not sarcastic.
We divided the dataset into two parts, training dataset and test dataset.
The classfier uses the word-vector model and the classifier is trained using the training dataset over 30 epochs.
The fully trained model is then fed with the test dataset.

The accuracy of the classifier over 30 epochs is 0.8128. We see that the loss goes down 
